{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfAJhrgFo5Tv"
      },
      "source": [
        "# Topic Modelling on store reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vt8L5e0mss0"
      },
      "source": [
        "NLP basically works in improving the unstructured data and it resolves ambiguity in language and so mainly its application works under Marketting and in advertising field. Marketers and Advertisers are our basic audience.  \n",
        "\n",
        "Businesses are heavily utilizing NLP techniques to analyze posts and understand their customers' profiles and requirements. NLP has also made it easier for companies to understand customer pain points.  \n",
        "\n",
        "Topic modelling is one of the unsupervised machine learning techniques which uses clustering to discover latent variables or hidden structures in the data. It is a method for locating topics in enormous amounts of text, in other words.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ9ZF9pQqW2X"
      },
      "source": [
        "We\"re gonna build an NLP pipeline with Spark NLP and train a topic model with PySpark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIszqyp8lzSU"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMxW9ewNmaPX"
      },
      "source": [
        "The initial task is to install PySpark and Spark NLP libraries to our Google Collaboratory and set the data up. We can proceed with the following installation. We also install `nltk` package because we will need it in one of the steps of our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yPmFVedSA68v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3e146f-4a2a-4a2f-a207-9d3c1571c63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_352\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_352-8u352-ga-1~20.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.352-b08, mixed mode)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Using cached pyspark-3.3.1-py2.py3-none-any.whl\n",
            "Collecting py4j==0.10.9.5\n",
            "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spark-nlp\n",
            "  Using cached spark_nlp-4.2.7-py2.py3-none-any.whl (453 kB)\n",
            "Installing collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-4.2.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp\n",
        "\n",
        "# Install nltk\n",
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAJnPrPL9qWU",
        "outputId": "8eed9a93-ba89-4e1c-a583-0e5d08d95b81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D07dyhss2Hk"
      },
      "source": [
        "The data is ready to use. Let's start the Spark session through Spark NLP. If you need a special Spark session setting, you can start Spark session with `SparkSession.builder` from PySpark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_E4acZCg8Ku8"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdZPmaFPtwyq"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfgD9jhRrFEF"
      },
      "source": [
        "We are going to use data from kaggle for our topic modelling analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twlcj7xFt318"
      },
      "source": [
        "First, we access the downloaded data and read it into Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "19uyI_8dAd3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6818ee9-3e19-4435-cda5-215aa43ea607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Id   ProductId          UserId                      ProfileName  \\\n",
            "0         1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
            "1         2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
            "2         3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
            "3         4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
            "4         5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
            "...     ...         ...             ...                              ...   \n",
            "9995   9996  B000P41A28  A3A63RACXR1XIL            A. Boodhoo \"deaddodo\"   \n",
            "9996   9997  B000P41A28    A5VVRGL8JA7R                             Adam   \n",
            "9997   9998  B000P41A28  A2TGDTJ8YCU6PD                          geena77   \n",
            "9998   9999  B000P41A28   AUV4GIZZE693O              Susan Coe \"sueysis\"   \n",
            "9999  10000  B000P41A28   A82WIMR4RSVLI                       Emrose mom   \n",
            "\n",
            "      HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
            "0                        1                       1      5  1303862400   \n",
            "1                        0                       0      1  1346976000   \n",
            "2                        1                       1      4  1219017600   \n",
            "3                        3                       3      2  1307923200   \n",
            "4                        0                       0      5  1350777600   \n",
            "...                    ...                     ...    ...         ...   \n",
            "9995                    10                      15      1  1204502400   \n",
            "9996                     2                       3      5  1306368000   \n",
            "9997                     0                       0      5  1347494400   \n",
            "9998                     1                       2      5  1203638400   \n",
            "9999                     0                       1      4  1337472000   \n",
            "\n",
            "                               Summary  \\\n",
            "0                Good Quality Dog Food   \n",
            "1                    Not as Advertised   \n",
            "2                \"Delight\" says it all   \n",
            "3                       Cough Medicine   \n",
            "4                          Great taffy   \n",
            "...                                ...   \n",
            "9995                      constipation   \n",
            "9996  Constipation Not A Problem if...   \n",
            "9997                Love this formula!   \n",
            "9998                   very convenient   \n",
            "9999        The best weve tried so far   \n",
            "\n",
            "                                                   Text  \n",
            "0     I have bought several of the Vitality canned d...  \n",
            "1     Product arrived labeled as Jumbo Salted Peanut...  \n",
            "2     This is a confection that has been around a fe...  \n",
            "3     If you are looking for the secret ingredient i...  \n",
            "4     Great taffy at a great price.  There was a wid...  \n",
            "...                                                 ...  \n",
            "9995  we switched from the advance similac to the or...  \n",
            "9996  Like the bad reviews say, the organic formula ...  \n",
            "9997  I wanted to solely breastfeed but was unable t...  \n",
            "9998  i love the fact that i can get this delieved t...  \n",
            "9999  We have a 7 week old... He had gas and constip...  \n",
            "\n",
            "[10000 rows x 10 columns]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Reviews.csv\"\n",
        "data = spark.read.csv(data_path, header=True)\n",
        "import pandas as pd\n",
        "df = pd.read_csv(data_path)\n",
        "df = df.head(10000)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BohpGAxPuezM"
      },
      "source": [
        "Let's checkout what kind of information is stored in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x_4_zyfNBlzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4621e7a1-4df5-4b20-b0e2-7e35e35d4e7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Id',\n",
              " 'ProductId',\n",
              " 'UserId',\n",
              " 'ProfileName',\n",
              " 'HelpfulnessNumerator',\n",
              " 'HelpfulnessDenominator',\n",
              " 'Score',\n",
              " 'Time',\n",
              " 'Summary',\n",
              " 'Text']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the size of our dataset\n",
        "\n",
        "data.count()"
      ],
      "metadata": {
        "id": "Rq2Mooeixznv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64880165-4ff9-43cc-c0d5-9663bcbc84c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "568454"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will be trimming our dataset to save on the resources\n",
        "\n",
        "data = data.limit(10000)\n",
        "data.count()"
      ],
      "metadata": {
        "id": "3loiMg6xyme-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bad7316-9f62-4cb6-bd71-083efebf1d0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2yf6V87uk9F"
      },
      "source": [
        "For topic modelling, we need only textual data, thus, we create a new dataframe only with the column of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h71u8qFR7g40"
      },
      "outputs": [],
      "source": [
        "text_col = \"Text\"\n",
        "review_text = data.select(text_col).filter(F.col(text_col).isNotNull())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quLpRWAEu2Gd"
      },
      "source": [
        "The data that we will use further for the analysis looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cID1bvIuyWgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e67abbd-4440-4cb7-d412-6ccba5602b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                Text|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "|I have bought several of the Vitality canned dog food products and have found them all to be of g...|\n",
            "|\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted...|\n",
            "|\"This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelati...|\n",
            "|If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this...|\n",
            "|Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very qui...|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "review_text.limit(5).show(truncate=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hHCLbrpfSqO"
      },
      "source": [
        "## Spark NLP pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i36XwEh8vL85"
      },
      "source": [
        "Here we start our NLP pipeline for the task of topic modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa4UrSFFvizm"
      },
      "source": [
        "### Basic NLP pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELN2q7q-vppo"
      },
      "source": [
        "Let's start with basic NLP pipeline that clears the data and gets lemmatized unigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PjxUquwv83c"
      },
      "source": [
        "We will start with **DocumentAssembler** that converts data into Spark NLP annotation format that can be used by Spark NLP annotators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XwOdzQ_PAJi0"
      },
      "outputs": [],
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol(text_col) \\\n",
        "     .setOutputCol(\"document\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "facUm7iMwUKZ"
      },
      "source": [
        "Next step is to tokenize data with **Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IGOLmEzJDCYW"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "     .setInputCols([\"document\"]) \\\n",
        "     .setOutputCol(\"tokenized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA98q-HCxVjb"
      },
      "source": [
        "Further, we clean our data and lowercase it with **Normalizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dTuQv1VXD8xu"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import Normalizer\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "     .setInputCols([\"tokenized\"]) \\\n",
        "     .setOutputCol(\"normalized\") \\\n",
        "     .setLowercase(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80FaYj0axg1r"
      },
      "source": [
        "We are going to lemmatize our text with pretrained lemming model provided by Spark NLP. We can access this model with **LemmatizerModel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a1lvJInOEEAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2130aecd-0358-470d-bb3a-4b5753306a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.annotator import LemmatizerModel\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "     .setInputCols([\"normalized\"]) \\\n",
        "     .setOutputCol(\"lemmatized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-fkFtA9yNEc"
      },
      "source": [
        "Spark NLP doesn\"t provide stop word list, hence, we will use `nltk` package to download stop words for English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Um9iOifSft2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf70fae0-2a2e-4293-d88f-b0cbaef67b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "eng_stopwords = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKDZh-kkybKt"
      },
      "source": [
        "The downloaded list of stop words we will input into **StopWordsCleaner** that will remove all such words from our lemmatized text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RJOGb5argbrb"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import StopWordsCleaner\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "     .setInputCols([\"lemmatized\"]) \\\n",
        "     .setOutputCol(\"unigrams\") \\\n",
        "     .setStopWords(eng_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW87_1g2yr3-"
      },
      "source": [
        "In addition to unigrams, it is good to use n-grams for topic modelling as well since they help to better refine topics. We can get n-grams with **NGramGenerator** in Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OsftJKunZO84"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import NGramGenerator\n",
        "\n",
        "ngrammer = NGramGenerator() \\\n",
        "    .setInputCols([\"lemmatized\"]) \\\n",
        "    .setOutputCol(\"ngrams\") \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter(\"_\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymL8JUPrzV0R"
      },
      "source": [
        "We already have our basic NLP pipeline for topic modelling with all necessary steps. However, let's use POS tagger in order to improve our processed data for topic modelling even more with POS tagged data later. For this, we are going to use pretrained POS tagging model provided by Spark NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ilOwKkC20Xe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe88dcd-1afd-463d-be6d-1ed10ed07132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 3.9 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.annotator import PerceptronModel\n",
        "\n",
        "pos_tagger = PerceptronModel.pretrained(\"pos_anc\") \\\n",
        "    .setInputCols([\"document\", \"lemmatized\"]) \\\n",
        "    .setOutputCol(\"pos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl6Xs_zE0Fc8"
      },
      "source": [
        "Now we have everything in Spark NLP annotation format. To be able to process the data further, we need to tranform data with **Finisher**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g3eYHx-KrDxU"
      },
      "outputs": [],
      "source": [
        "from sparknlp.base import Finisher\n",
        "\n",
        "finisher = Finisher().setInputCols([\"unigrams\", \"ngrams\", \"pos\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP-kJKHQ0T8e"
      },
      "source": [
        "Now we are ready to input everything into a pipeline. **Pipeline** functionality is accessible with PySpark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Oo4ixwfsrMPh"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline() \\\n",
        "     .setStages([documentAssembler,                  \n",
        "                 tokenizer,\n",
        "                 normalizer,                  \n",
        "                 lemmatizer,                  \n",
        "                 stopwords_cleaner, \n",
        "                 pos_tagger,\n",
        "                 ngrammer,  \n",
        "                 finisher])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yXiHvEE0gYx"
      },
      "source": [
        "First, we will fit all our estimators and then, transform the data with trained models and transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pCkceTwhrOf0"
      },
      "outputs": [],
      "source": [
        "processed_review = pipeline.fit(review_text).transform(review_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ovKGdr0p6J"
      },
      "source": [
        "Let's look at the data we finally get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D-vuMJZD9Lwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a49cc69-acad-4239-e07a-0cfcbc5a5082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                Text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|I have bought sev...|[buy, several, vi...|[i, have, buy, se...|[NNP, VBP, VBN, J...|\n",
            "|\"Product arrived ...|[product, arrive,...|[product, arrive,...|[NN, JJ, NN, IN, ...|\n",
            "|\"This is a confec...|[confection, arou...|[this, be, a, con...|[DT, VB, DT, NN, ...|\n",
            "|If you are lookin...|[look, secret, in...|[if, you, be, loo...|[IN, PRP, VB, VB,...|\n",
            "|Great taffy at a ...|[great, taffy, gr...|[great, taffy, at...|[JJ, NN, IN, DT, ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review.limit(5).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Oxvz01vfaq6"
      },
      "source": [
        "### Extended NLP pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNShG0il00g6"
      },
      "source": [
        "We have our data in a form of unigrams that are lemmatized, with no stop words in there. It is a good idea to incorporate n-grams into our NLP pipeline. We obtained n-grams as one step of our pipeline but now n-grams are messy and have a lot of questionable combinations in there. To tackle this problem, let\"s filter out strange combinations of words in n-grams based on their POS tags. We can imagine a list of viable combinations like ADJ + NOUN so let\"s restrict our POS combinations in n-grams to this list. Plus, we can also exclude some POS tags from our unigrams to ensure that we don\"t use functional words for topic modelling (they can be partially covered by stop words but probably not fully).\n",
        "\n",
        "Doing this POS-based filtering will significantly reduce the vocabulary size for topic modelling which will speed up the whole processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FnnKmIf2Ssc"
      },
      "source": [
        "For this processing, first, we need join all our POS tags obtained previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WbPpOs8ndGE9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import types as T\n",
        "\n",
        "udf_join_arr = F.udf(lambda x: \" \".join(x), T.StringType())\n",
        "processed_review  = processed_review.withColumn(\"finished_pos\", udf_join_arr(F.col(\"finished_pos\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzSryqMc2an7"
      },
      "source": [
        "Then we start another Spark NLP pipeline in order to get POS tag n-grams that correspond to word n-grams. We start with convertation into Spark NLP annotation format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RrKEpmfoba6M"
      },
      "outputs": [],
      "source": [
        "pos_documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol(\"finished_pos\") \\\n",
        "     .setOutputCol(\"pos_document\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smZ6-1dz2uf3"
      },
      "source": [
        "Then, we tokenize our POS tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pyZdZ7S8aGZ9"
      },
      "outputs": [],
      "source": [
        "pos_tokenizer = Tokenizer() \\\n",
        "     .setInputCols([\"pos_document\"]) \\\n",
        "     .setOutputCol(\"pos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8kJmdm52w-M"
      },
      "source": [
        "And generate n-grams from them in the same way we did that for words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9QnL0zwrW_w-"
      },
      "outputs": [],
      "source": [
        "pos_ngrammer = NGramGenerator() \\\n",
        "    .setInputCols([\"pos\"]) \\\n",
        "    .setOutputCol(\"pos_ngrams\") \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter(\"_\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG3UTh0N23Yf"
      },
      "source": [
        "Lastly, we are ready to get POS tags ngrams with **Finisher**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CiUGFlMHZqIZ"
      },
      "outputs": [],
      "source": [
        "pos_finisher = Finisher().setInputCols([\"pos\", \"pos_ngrams\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSQ1XeNZ2--r"
      },
      "source": [
        "We create this new Spark NLP pipeline..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_-AufeTzbYwM"
      },
      "outputs": [],
      "source": [
        "pos_pipeline = Pipeline() \\\n",
        "     .setStages([pos_documentAssembler,                  \n",
        "                 pos_tokenizer,\n",
        "                 pos_ngrammer,  \n",
        "                 pos_finisher])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQJcpiA3EBg"
      },
      "source": [
        "... and again fit it and transform the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0kfpx2azbmJI"
      },
      "outputs": [],
      "source": [
        "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUbDHzkZ3JzH"
      },
      "source": [
        "Let's look what kind of data we have to operate with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "x0dBA85zf3Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12a8206-a40d-4706-df16-59bb45b0c262"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Text',\n",
              " 'finished_unigrams',\n",
              " 'finished_ngrams',\n",
              " 'finished_pos',\n",
              " 'finished_pos_ngrams']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "processed_review.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5VXa6ir3Q97"
      },
      "source": [
        "And these are our word n-grams with their corresponding pos n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dPRHCY32eKAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e182b14-fe5b-4154-f59a-855205dc679b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|     finished_ngrams| finished_pos_ngrams|\n",
            "+--------------------+--------------------+\n",
            "|[i, have, buy, se...|[NNP, VBP, VBN, J...|\n",
            "|[product, arrive,...|[NN, JJ, NN, IN, ...|\n",
            "|[this, be, a, con...|[DT, VB, DT, NN, ...|\n",
            "|[if, you, be, loo...|[IN, PRP, VB, VB,...|\n",
            "|[great, taffy, at...|[JJ, NN, IN, DT, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review.select(\"finished_ngrams\", \"finished_pos_ngrams\").limit(5).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCXs_3yNfXbG"
      },
      "source": [
        "Now we are ready to filter out not useful for topic modelling analysis POS tags from our data. Let's create the function that does it for unigrams first. We create the custom Python function and then transform it to PySpark UDF to be used on Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "YhhKK97NfEbd"
      },
      "outputs": [],
      "source": [
        "def filter_pos(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if pos in [\"JJ\", \"NN\", \"NNS\", \"VB\", \"VBP\"]]\n",
        "\n",
        "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_hy63My3rrV"
      },
      "source": [
        "Then, we apply this function on columns with unigrams and their POS tags to get filtered unigrams in a separate dataframe column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rEx5CHyqfWeG"
      },
      "outputs": [],
      "source": [
        "processed_review = processed_review.withColumn(\"filtered_unigrams\",\n",
        "                                               udf_filter_pos(F.col(\"finished_unigrams\"), \n",
        "                                                              F.col(\"finished_pos\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh1XVgPL4K6G"
      },
      "source": [
        "That is how our filtered unigrams look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "L22iUDgWgMVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bafc9b7-7fcf-4c0e-a083-6d3c4c459a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                         filtered_unigrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[several, dog, find, quality, product, look, stew, process, labrador, appreciate, product]|\n",
            "|[product, arrive, label, salt, peanutsthe, peanut, actually, small, unsalted, sure, err...|\n",
            "|[around, light, citrus, gelatin, filbert, cut, square, coat, powder, sugar, tiny, heave...|\n",
            "|             [ingredient, robitussin, get, addition, beer, order, make, cherry, medicinal]|\n",
            "|                               [great, taffy, wide, assortment, taffy, quick, taffy, deal]|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review.select(\"filtered_unigrams\").limit(5).show(truncate=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEDG4jfD4Rux"
      },
      "source": [
        "It is time to filter out improper POS combinations of n-grams. We create the custom function in the same manner as before. Since we deal with bi- and trigrams, we need to restrict tags for both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yJd-nLnxgd1g"
      },
      "outputs": [],
      "source": [
        "def filter_pos_combs(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if (len(pos.split(\"_\")) == 2 and \\\n",
        "                pos.split(\"_\")[0] in [\"JJ\", \"NN\", \"NNS\", \"VB\", \"VBP\"] and \\\n",
        "                 pos.split(\"_\")[1] in [\"JJ\", \"NN\", \"NNS\"]) \\\n",
        "            or (len(pos.split(\"_\")) == 3 and \\\n",
        "                pos.split(\"_\")[0] in [\"JJ\", \"NN\", \"NNS\", \"VB\", \"VBP\"] and \\\n",
        "                 pos.split(\"_\")[1] in [\"JJ\", \"NN\", \"NNS\", \"VB\", \"VBP\"] and \\\n",
        "                  pos.split(\"_\")[2] in [\"NN\", \"NNS\"])]\n",
        "    \n",
        "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zjj8b9v425f"
      },
      "source": [
        "And we call the function on word and POS n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QH1gNrz6giPU"
      },
      "outputs": [],
      "source": [
        "processed_review = processed_review.withColumn(\"filtered_ngrams\",\n",
        "                                               udf_filter_pos_combs(F.col(\"finished_ngrams\"),\n",
        "                                                                    F.col(\"finished_pos_ngrams\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kA7epRY5HYy"
      },
      "source": [
        "Below is what we get after filtering for n-grams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "gcXY0eMChtTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5667851-5915-48ad-dbe8-05182960b095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                           filtered_ngrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[dog_food, food_product, good_quality, product_look, process_meat, be_finicky, dog_food...|\n",
            "|[product_arrive, arrive_label, jumbo_salt, salt_peanutsthe, peanutsthe_peanut, small_si...|\n",
            "|[few_century, light_pillowy, pillowy_citrus, citrus_gelatin, case_filbert, tiny_square,...|\n",
            "|     [secret_ingredient, root_beer, beer_extract, be_good, cherry_soda, root_beer_extract]|\n",
            "|[great_taffy, great_price, wide_assortment, yummy_taffy, taffy_delivery, taffy_lover, y...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review.select(\"filtered_ngrams\").limit(5).show(truncate=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciCceJZH5NmN"
      },
      "source": [
        "Now we have unigrams and n-grams stored in different columns in the dataframe. Let's combine them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GVNl50pI7U1S"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import concat\n",
        "\n",
        "processed_review = processed_review.withColumn(\"final\", \n",
        "                                               concat(F.col(\"filtered_unigrams\"), \n",
        "                                                      F.col(\"filtered_ngrams\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jxNhhjb5ZOt"
      },
      "source": [
        "And this is our final look of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "k5HOIIzniFzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b795e2b-30b4-4d35-aeac-52323bebd0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                     final|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[several, dog, find, quality, product, look, stew, process, labrador, appreciate, produ...|\n",
            "|[product, arrive, label, salt, peanutsthe, peanut, actually, small, unsalted, sure, err...|\n",
            "|[around, light, citrus, gelatin, filbert, cut, square, coat, powder, sugar, tiny, heave...|\n",
            "|[ingredient, robitussin, get, addition, beer, order, make, cherry, medicinal, secret_in...|\n",
            "|[great, taffy, wide, assortment, taffy, quick, taffy, deal, great_taffy, great_price, w...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "processed_review.select(\"final\").limit(5).show(truncate=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OTK4DVHxcR0"
      },
      "source": [
        "## Vectorization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP4HlvFR5jXp"
      },
      "source": [
        "Now, we are set to vectorization of our data. First, we will proceed with **TF** (*term frequency*) vectorization with **CountVectorizer** in PySpark. We fit tf dictionary and then transform the data to vectors of counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tqGT4ss7r_w2"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tfizer = CountVectorizer(inputCol=\"final\", outputCol=\"tf_features\")\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "217ZJrT76B2h"
      },
      "source": [
        "After we get TF results, we can account for words that are frequent for all the documents. We can use **IDF** (*inverse document frequency*) to lower score of such words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ox3dEUoyx2Ss"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "idfizer = IDF(inputCol=\"tf_features\", outputCol=\"tf_idf_features\")\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezlCICGwzvQu"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kelkPspY6okR"
      },
      "source": [
        "Finally, we are ready to model topics in our data with **LDA** (*Latent Dirichlet Allocation*). To use the algorithm, we have to provide the number of topics we presume our data contains and the number of iterations for the LDA algorithm. Then, we initialize the model and train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ID3lf4GjzxJq"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 3\n",
        "max_iter = 100\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol=\"tf_idf_features\")\n",
        "lda_model = lda.fit(tfidf_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DslMpvbT7GXQ"
      },
      "source": [
        "To be able to see words that characterize the defined topics, we need to convert word ids into actual words with the custom function. This function will again be converted to PySpark UDF to be used on our topic dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OxbhwXnQ0dEa"
      },
      "outputs": [],
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GllCV-hw7fgL"
      },
      "source": [
        "Let's define the number of top words per topic we would like to see and extract the words with our function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Fhsk4J5p0lOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06b41bd-f2c1-4755-c987-d9aef31f43ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------------------------------------------------------+\n",
            "|topic|                                                     topicWords|\n",
            "+-----+---------------------------------------------------------------+\n",
            "|    0|[taste, good, like, chip, flavor, br, product, great, use, try]|\n",
            "|    1| [br, food, dog, use, make, dog_food, product, good, like, one]|\n",
            "|    2|    [coffee, tea, cup, like, taste, good, try, flavor, one, br]|\n",
            "+-----+---------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_top_words = 10\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn(\"topicWords\", udf_to_words(F.col(\"termIndices\")))\n",
        "topics.select(\"topic\", \"topicWords\").show(truncate=90)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f300e0ab555480293fa744b19a6dbf33ecb6efb7d88051cdfdbadd3b6a514546"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
